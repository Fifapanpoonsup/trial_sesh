{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim import corpora, models\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_soup = BeautifulSoup(open(r\"C:\\Users\\User\\Desktop\\thesis_17_3_2022\\stock_price_and_news\\stock_news\\dataset\\urbn_final\\urbn_2003.html\", encoding=\"utf8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_timestamp = []\n",
    "for e in urbn_soup.find_all(\"span\", {\"class\": \"article__timestamp\"}):\n",
    "    urbn_timestamp.append(e.get_text())\n",
    "urbn_file = []\n",
    "for gps_text in urbn_soup.find_all(\"h3\"):\n",
    "    urbn_file.append(gps_text.get_text())\n",
    "urbn_clean1 = []\n",
    "for gps_text1 in urbn_file:\n",
    "    urbn_clean1.append(gps_text1.replace(\"\\n\", \"\"))\n",
    "urbn_remove = []\n",
    "for e in urbn_clean1:\n",
    "    if e not in (\n",
    " 'No Recent Tickers',\n",
    " 'Overview'\n",
    " ):\n",
    "        urbn_remove.append(e)\n",
    "urbn_space = []\n",
    "for e in urbn_remove:\n",
    "    urbn_space.append(e.strip())\n",
    "urbn_fullstop = []\n",
    "for e in urbn_space:\n",
    "    urbn_fullstop.append(e.replace(\".\",\"\"))\n",
    "urbn_q = []\n",
    "for e in urbn_fullstop:\n",
    "    urbn_q.append(e.replace(\"?\",\"\"))\n",
    "urbn_colon = []\n",
    "for e in urbn_q:\n",
    "    urbn_colon.append(e.replace(\":\",\"\"))\n",
    "urbn_comma = []\n",
    "for e in urbn_colon:\n",
    "    urbn_comma.append(e.replace(\",\",\"\"))\n",
    "urbn_percent = []\n",
    "for e in urbn_comma:\n",
    "    urbn_percent.append(e.replace(\"%\",\"percent\"))\n",
    "urbn_lower = []\n",
    "for i in range(len(urbn_percent)):\n",
    "    urbn_lower.append(urbn_percent[i].lower())\n",
    "urbn_barr = []\n",
    "for e in urbn_lower:\n",
    "    urbn_barr.append(e.replace(\"barron's\",\"\"))\n",
    "urbn_pain = []\n",
    "for e in urbn_barr:\n",
    "    if e not in ( 'marketwatch',\n",
    " 'company',\n",
    " 'dow jones network'):\n",
    "        urbn_pain.append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_et = []\n",
    "for e in urbn_timestamp:\n",
    "    urbn_et.append(e.replace(\"ET\",\"\"))\n",
    "urbn_at1 = []\n",
    "for e in urbn_et:\n",
    "    urbn_at1.append(e.replace(\"at\",\"\"))\n",
    "urbn_pm1 = []\n",
    "for e in urbn_at1:\n",
    "    urbn_pm1.append(e.replace(\"p.m.\",\"\"))\n",
    "urbn_am1 = []\n",
    "for e in urbn_pm1:\n",
    "    urbn_am1.append(e.replace(\"a.m.\",\"\"))\n",
    "urbn_commas = []\n",
    "for e in urbn_am1:\n",
    "    urbn_commas.append(e.replace(\",\",\"\"))\n",
    "urbn_fullstops = []\n",
    "for e in urbn_commas:\n",
    "    urbn_fullstops.append(e.replace(\".\",\"\"))\n",
    "urbn_timestamp_final = urbn_fullstops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feb 22 2022  1:46  ',\n",
       " 'Feb 18 2022  7:38  ',\n",
       " 'Feb 15 2022  10:38  ',\n",
       " 'Feb 13 2022  9:55  ',\n",
       " 'Feb 11 2022  3:38  ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbn_timestamp_final[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_date = [datetime.strptime(x,'%b %d %Y  %I:%M ') for x in urbn_timestamp_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_data = pd.DataFrame(\n",
    "    {'Date': urbn_date,\n",
    "     'text_headlines': urbn_pain\n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_data.to_csv('urbn_topic_selection.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbn_data.head(3)\n",
    "urbn_news_matrix = tf_vectorizer.fit_transform(urbn_data['text_headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  ...\n",
       "1                                                  ...\n",
       "2    urban outfitters shares slip as free people pr...\n",
       "3    urban outfitters stock slides 39percent premarket\n",
       "4    urban outfitters says q4 margins hurt by highe...\n",
       "Name: news_text_processed, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "urbn_data['news_text_processed'] = urbn_data['text_headlines'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "urbn_data['news_text_processed'] = urbn_data['text_headlines'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "urbn_data['news_text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retail', 'stocks', 'to', 'avoid', 'amid', 'inflation', 'uncertainty', 'according', 'to', 'barclays']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data = urbn_data.news_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['retail', 'stock', 'avoid', 'inflation', 'uncertainty', 'accord', 'barclay']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.081*\"outfitter\" + 0.081*\"urban\" + 0.036*\"stock\" + 0.027*\"miss\" + '\n",
      "  '0.023*\"drop\" + 0.022*\"sale\" + 0.019*\"jump\" + 0.013*\"profit\" + 0.011*\"say\" + '\n",
      "  '0.010*\"result\"'),\n",
      " (1,\n",
      "  '0.060*\"urban\" + 0.060*\"outfitter\" + 0.036*\"sector\" + 0.035*\"retail\" + '\n",
      "  '0.032*\"stock\" + 0.029*\"upgrade\" + 0.016*\"lead\" + 0.016*\"buy\" + '\n",
      "  '0.015*\"wal_mart\" + 0.012*\"neutral\"'),\n",
      " (2,\n",
      "  '0.071*\"outfitter\" + 0.071*\"urban\" + 0.060*\"surprise\" + '\n",
      "  '0.053*\"updates_advisorie\" + 0.035*\"cent\" + 0.021*\"ep\" + 0.016*\"percent\" + '\n",
      "  '0.016*\"cut\" + 0.014*\"neutral\" + 0.011*\"buy\"'),\n",
      " (3,\n",
      "  '0.167*\"urban\" + 0.166*\"outfitter\" + 0.089*\"percent\" + 0.069*\"share\" + '\n",
      "  '0.054*\"sale\" + 0.026*\"break\" + 0.025*\"profit\" + 0.019*\"earning\" + '\n",
      "  '0.019*\"fall\" + 0.019*\"rise\"'),\n",
      " (4,\n",
      "  '0.119*\"urban\" + 0.119*\"outfitter\" + 0.091*\"stock\" + 0.082*\"price\" + '\n",
      "  '0.081*\"target\" + 0.054*\"cut\" + 0.036*\"raise\" + 0.010*\"earning\" + '\n",
      "  '0.010*\"mkm_partner\" + 0.009*\"estimate\"'),\n",
      " (5,\n",
      "  '0.042*\"retailer\" + 0.037*\"stock\" + 0.021*\"outfitter\" + 0.021*\"urban\" + '\n",
      "  '0.015*\"opinion\" + 0.014*\"earning\" + 0.013*\"close\" + 0.013*\"lose\" + '\n",
      "  '0.013*\"retail\" + 0.013*\"streak\"'),\n",
      " (6,\n",
      "  '0.029*\"quarter\" + 0.028*\"hour\" + 0.024*\"urban\" + 0.023*\"outfitter\" + '\n",
      "  '0.022*\"share\" + 0.022*\"market\" + 0.021*\"premium\" + 0.017*\"rise\" + '\n",
      "  '0.016*\"rally\" + 0.014*\"first\"'),\n",
      " (7,\n",
      "  '0.068*\"stock\" + 0.039*\"urban\" + 0.037*\"outfitter\" + 0.026*\"retail\" + '\n",
      "  '0.025*\"buy\" + 0.016*\"good\" + 0.014*\"rise\" + 0.014*\"sale\" + 0.012*\"watch\" + '\n",
      "  '0.011*\"share\"'),\n",
      " (8,\n",
      "  '0.107*\"stock\" + 0.029*\"outfitter\" + 0.029*\"urban\" + 0.023*\"retail\" + '\n",
      "  '0.021*\"watch\" + 0.016*\"sale\" + 0.016*\"future\" + 0.014*\"decline\" + '\n",
      "  '0.013*\"earning\" + 0.013*\"rally\"'),\n",
      " (9,\n",
      "  '0.034*\"market\" + 0.033*\"outfitter\" + 0.033*\"urban\" + 0.033*\"percent\" + '\n",
      "  '0.022*\"stock\" + 0.019*\"month\" + 0.018*\"premarket\" + 0.017*\"retail\" + '\n",
      "  '0.016*\"sale\" + 0.016*\"money\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.46293407964310385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 327/540 [1:12:55<25:42,  7.24s/it]  C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n",
      "100%|██████████| 540/540 [2:11:44<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 6\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
